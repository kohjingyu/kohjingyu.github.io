<!doctype html>
<html lang="en">
  <head>
    <link rel="icon" href="favicon.ico">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <script src="https://use.fontawesome.com/23e9e06de4.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <link href="css/stylesheet.css" rel="stylesheet">

    <title>Jing Yu Koh</title>

      <!-- Google Analytics -->
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-15238452-2', 'auto');
        ga('send', 'pageview');

      </script>
  </head>

  <body>

    <!-- <header>
      <div class="navbar navbar-dark bg-dark box-shadow">
        <div class="container d-flex justify-content-between">
          <a href="#" class="navbar-brand d-flex align-items-center">
            <strong>Koh Jing Yu</strong>
          </a>
        </div>
      </div>
    </header> -->

    <main role="main">
    <div align="center">

      <section class="jumbotron header">
        <div class="col-md-8 row jumbotron-content">
        <div class="text-left col-md-9">
          <h1 class="jumbotron-heading text-center">Jing Yu <strong>Koh</strong></h1>
          <p align="center">jingyuk@cs.cmu.edu</p>
          <p>I am a PhD student in the <a href="https://www.ml.cmu.edu" target="_blank">Machine Learning Department</a> at Carnegie Mellon University, advised by <a href="https://dpfried.github.io" target="_blank">Daniel Fried</a> and <a href="https://www.cs.cmu.edu/~rsalakhu/index.html" target="_blank">Ruslan Salakhutdinov</a>. I do research on multimodal language model agents and other vision-and-language problems.</p>
          <p>Prior to this, I worked at <a href="https://research.google" target="_blank">Google Research</a> in <a href="https://scholar.google.com/citations?user=TP_JZm8AAAAJ" target="_blank">Jason Baldridge</a>'s team from 2019-2022, where I conducted research on vision-and-language problems and generative models. Before that, I completed my undergraduate studies at the <a href="https://sutd.edu.sg" target="_blank">Singapore University of Technology and Design</a> <i>summa cum laude</i> (highest honors) in 2019.</p>

          <p class="note">My first name is "Jing Yu" and informally I go by the nickname "JY". 许靖宇 is my name in Chinese. I'm from Singapore.</p>
        <div class="text-center">
          <p>
            <a href="https://twitter.com/kohjingyu" target="_blank"><i class="fa fa-twitter contact-icon" aria-hidden="true"></i></a>
            <a href="https://scholar.google.com/citations?user=iGLKl7cAAAAJ" target="_blank"><i class="ai ai-google-scholar contact-icon" aria-hidden="true"></i></a>
            <!-- <a href="https://github.com/kohjingyu" target="_blank"><i class="fa fa-github contact-icon" aria-hidden="true"></i></a> -->
            <!-- <a href="https://www.linkedin.com/in/kohjingyu/" target="_blank"><i class="fa fa-linkedin-square contact-icon" aria-hidden="true"></i></a> -->
            <a href="mailto:jingyuk@cs.cmu.edu"><i class="fa fa-envelope contact-icon" aria-hidden="true"></i></a>
          </p>
        </div>
        </div>
        <div class="col-md-3">
        <img class="profile-img" src="images/photo.jpg">
        </div>
        </div>
      </section>



      <div class="album py-5 col-md-9 text-left news-div">
        <!-- News -->
        <div class="container">
        <h2>News</h2>
        <hr>
        <ul>
        <li><span class="news-date">(Jun 2024)</span> Excited to share a new preprint: <a href="https://jykoh.com/search-agents" target="_blank">Tree Search for Language Model Agents</a>.</li>
        <li><span class="news-date">(Spring 2024)</span> Gave invited talks about <a href="https://jykoh.com/vwa" target="_blank">VisualWebArena</a> at NUS, Jane Street, and Cohere for AI (<a href="https://www.youtube.com/watch?v=PmWDEqkVnqg" target="_blank">recording</a>).</li>
        <li><span class="news-date">(Feb 2024)</span> Awarded the <a href="https://www.janestreet.com/join-jane-street/programs-and-events/grf-profiles-2024/" target="_blank">Jane Street Graduate Research Fellowship</a>. Thank you Jane Street!</li>
        <li><span class="news-date">(Jan 2024)</span> Excited to share a new preprint: <a href="https://jykoh.com/vwa" target="_blank">VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks</a>.</li>
        <li><span class="news-date">(Sep 2023)</span> 1 paper accepted to <a href="https://neurips.cc" target="_blank">NeurIPS 2023</a>!</li>
        <li><span class="news-date">(Summer 2023)</span> Gave an invited talk about <a href="/gill" target="_blank">GILL</a> at <a href="https://mlcollective.org/dlct/" target="_blank">DLCT</a> and <a href="https://cohere.for.ai/" target="_blank">Cohere For AI</a> (<a href="/gill/gill_talk.pdf" target="_blank">slides</a>, <a href="https://www.youtube.com/watch?v=aUoiQ4xFknQ" target="_blank">recording</a>).</li>
        <li><span class="news-date">(Apr 2023)</span> 1 paper accepted to <a href="https://icml.cc" target="_blank">ICML 2023</a>!</li>
        <li><span class="news-date">(Spring 2023)</span> Gave invited talks at Microsoft Research, Apple AI/ML, Georgia Tech, and the London ML Meetup (<a href="https://www.youtube.com/watch?v=WNIjb96GvxU" target="_blank">recording</a>, <a href="fromage/fromage_talk.pdf" target="_blank">slides</a>).</li>

        <div class="wrap-collabsible">
          <input id="collapsible" class="toggle" type="checkbox">
          <label for="collapsible" class="lbl-toggle">Older news</label>
          <div class="collapsible-content">
            <div class="content-inner">
        <!--         <li><span class="news-date">(Jan 2023)</span> <a href="https://arxiv.org/abs/2301.13823" target="_blank">New preprint</a>! We ground LLMs to enable multimodal processing and generation.</li>
         -->        <li><span class="news-date">(Dec 2022)</span> I made a <a href="https://benchugg.com/writing/ai-bet/" target="_blank">bet on LLM capabilities</a> with my office mate Ben Chugg. Bubble tea is on the line.</li>
                <!-- <li><span class="news-date">(Dec 2022)</span> 1 paper accepted to <a hef="https://aaai.org/Conferences/AAAI-23/" target="_blank">AAAI 2023</a>.</li> -->
                <li><span class="news-date">(Nov 2022)</span> <a href="https://parti.research.google" target="_blank">Parti</a> was accepted to TMLR with a Featured Certification!</li>
                <li><span class="news-date">(Oct 2022)</span> In the spirit of paying it forward, I'm sharing my <a href="jykoh_sop.pdf" target="_blank">Statement of Purpose</a> publicly. Hope it helps future applicants!</li>
                <li><span class="news-date">(Jul 2022)</span> After 2.73 wonderful years at Google, I've left to pursue my PhD at Carnegie Mellon University!</li>
                <li><span class="news-date">(January 2022)</span> 1 paper accepted to <a href="https://iclr.cc" target="_blank">ICLR 2022</a>!</li>
                <li><span class="news-date">(December 2021)</span> Serving as a reviewer for <a href="https://cvpr2022.thecvf.com" target="_blank">CVPR 2022</a>.</li>
                <li><span class="news-date">(July 2021)</span> 1 paper accepted to <a href="http://iccv2021.thecvf.com" target="_blank">ICCV 2021</a>!</li>
                <li><span class="news-date">(July 2021)</span> Presenting an <a href="https://www.microsoft.com/en-us/research/video/grounded-visual-generation/" target="_blank">invited talk</a> at Microsoft Research.</li>
                <li><span class="news-date">(July 2021)</span> Serving as a reviewer for <a href="https://nips.cc/Conferences/2021/" target="_blank">NeurIPS 2021</a>.</li>
                <li><span class="news-date">(March 2021)</span> 1 paper accepted to <a href="http://cvpr2021.thecvf.com" target="_blank">CVPR 2021</a>!</li>
                <li><span class="news-date">(January 2021)</span> 1 paper accepted to <a href="https://iclr.cc" target="_blank">ICLR 2021</a>!</li>
                <li><span class="news-date">(October 2020)</span> 1 paper accepted to <a href="http://wacv2021.thecvf.com/" target="_blank">WACV 2021</a>!</li>
                <li><span class="news-date">(July 2020)</span> 1 paper accepted to <a href="https://eccv2020.eu" target="_blank">ECCV 2020</a>!</li>
                <li><span class="news-date">(October 2019)</span> Officially joined Google as an AI Resident in Mountain View, California.</li>
            </div>
          </div>
        </div>
        </ul>
        </div>
        <br/>

        <!-- Publications -->
        <div class="container">
        <h2>Selected Publications [<a href="https://scholar.google.com/citations?user=iGLKl7cAAAAJ" target="_blank">Google Scholar</a>]</h2>
        <hr>
        <h3>2024</h3>
        <div class="row publication-div mb-4 box-shadow">
        <a href="https://jykoh.com/search-agents/paper.pdf" alt="Tree Search for Language Model Agents" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="search-agents/search_overview_trunc.png" alt="" /></a>
        </div>
        </a>
        <div class="col-md-9">
        <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Tree Search for Language Model Agents
</p>
                <p class="paper-authors"><strong>Jing Yu Koh</strong>, Stephen McAleer, Daniel Fried, Ruslan Salakhutdinov</p>
                <p class="paper-venue"><i>Preprint</i>, 2024.</p>
                <!-- <p class="paper-venue">As seen on: <a href="https://www.wired.com/story/fast-forward-tested-next-gen-ai-assistant/" target="_blank">Wired</a>.</p> -->
            </div>
        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://jykoh.com/search-agents" target="_blank">Project Page</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://jykoh.com/search-agents/paper.pdf" target="_blank">PDF</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://github.com/kohjingyu/search-agents" target="_blank">Code & Data</a>
        </div>
        </div>
        </div>
        </div>

        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/abs/2401.13649" alt="VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="vwa/overview-small.png" alt="" /></a>
        </div>
        </a>
        <div class="col-md-9">
        <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks
</p>
                <p class="paper-authors"><strong>Jing Yu Koh</strong>, Robert Lo*, Lawrence Jang*, Vikram Duvvur*, Ming Chong Lim*, Po-Yu Huang*, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried</p>
                <p class="paper-venue"><i>ACL</i>, 2024.</p>
                <!-- <p class="paper-venue">As seen on: <a href="https://www.wired.com/story/fast-forward-tested-next-gen-ai-assistant/" target="_blank">Wired</a>.</p> -->
            </div>
        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://jykoh.com/vwa" target="_blank">Project Page</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/abs/2401.13649" target="_blank">PDF</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://github.com/web-arena-x/visualwebarena/" target="_blank">Code & Data</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://www.youtube.com/watch?v=PmWDEqkVnqg" target="_blank">Talk</a>
        </div>
        </div>
        </div>
        </div>


        <h3>2023</h3>
        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/abs/2305.17216" alt="Generating Images with Multimodal Language Models" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="gill/dialogue-small.png" alt="Generating Images with Multimodal Language Models" /></a>
        </div>
        </a>
        <div class="col-md-9">
        <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Generating Images with Multimodal Language Models</p>
                <p class="paper-authors"><strong>Jing Yu Koh</strong>, Daniel Fried, Ruslan Salakhutdinov</p>
                <p class="paper-venue"><i>NeurIPS</i>, 2023.</p>
            </div>
        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://jykoh.com/gill" target="_blank">Project Page</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/abs/2305.17216" target="_blank">PDF</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://github.com/kohjingyu/gill" target="_blank">Code</a>
        <a class="btn btn-sm btn-outline-secondary" href="gill/gill_talk.pdf" target="_blank">Slides</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://www.youtube.com/watch?v=aUoiQ4xFknQ" target="_blank">Talk</a>
        <!-- <a class="btn btn-sm btn-outline-secondary" href="https://twitter.com/kohjingyu/status/1663537830771998720" target="_blank">Twitter Thread</a> -->
        </div>
        </div>
        </div>
        </div>

        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/abs/2301.13823" alt="Grounding Language Models to Images for Multimodal Inputs and Outputs" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="fromage/bird_chat.png" alt="Grounding Language Models to Images for Multimodal Inputs and Outputs" /></a>
        </div>
        </a>
        <div class="col-md-9">
        <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Grounding Language Models to Images for Multimodal Inputs and Outputs</p>
                <p class="paper-authors"><strong>Jing Yu Koh</strong>, Ruslan Salakhutdinov, Daniel Fried</p>
                <p class="paper-venue"><i>ICML</i>, 2023.</p>
            </div>
        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://jykoh.com/fromage" target="_blank">Project Page</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/abs/2301.13823" target="_blank">PDF</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://github.com/kohjingyu/fromage" target="_blank">Code</a>
        <a class="btn btn-sm btn-outline-secondary" href="fromage/fromage_talk.pdf" target="_blank">Slides</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://www.youtube.com/watch?v=WNIjb96GvxU" target="_blank">Talk</a>
        <!-- <a class="btn btn-sm btn-outline-secondary" href="https://twitter.com/kohjingyu/status/1620602402813648898" target="_blank">Twitter Thread</a> -->
        </div>
        </div>
        </div>
        </div>


        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/abs/2302.06833" alt="VQ3D: Learning a 3D-Aware Generative Model on ImageNet" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/vq3d.png" alt="VQ3D: Learning a 3D-Aware Generative Model on ImageNet" /></a>
        </div>
        </a>
        <div class="col-md-9">
        <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">VQ3D: Learning a 3D-Aware Generative Model on ImageNet</p>
                <p class="paper-authors">Kyle Sargent, <strong>Jing Yu Koh</strong>, Han Zhang, Huiwen Chang, Charles Herrmann, Pratul Srinivasan, Jiajun Wu, Deqing Sun</p>
                <p class="paper-venue"><i>ICCV (oral, best paper finalist)</i>, 2023.</p>
            </div>
        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://kylesargent.github.io/vq3d" target="_blank">Project Page</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/abs/2302.06833" target="_blank">PDF</a>
        </div>
        </div>
        </div>
        </div>


        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/abs/2204.02960" alt="Simple and Effective Synthesis of Indoor 3D Scenes" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/se3ds.png" alt="Simple and Effective Synthesis of Indoor 3D Scenes" /></a>
        </div>
        </a>
        <div class="col-md-9">
        <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Simple and Effective Synthesis of Indoor 3D Scenes</p>
                <p class="paper-authors"><strong>Jing Yu Koh*</strong>, Harsh Agrawal*, Dhruv Batra, Richard Tucker, Austin Waters, Honglak Lee, Yinfei Yang, Jason Baldridge, Peter Anderson (* denotes equal contribution)</p>
                <p class="paper-venue"><i>AAAI</i>, 2023.</p>
            </div>
        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/abs/2204.02960" target="_blank">PDF</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://github.com/google-research/se3ds" target="_blank">Code</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://www.youtube.com/watch?v=4fVG0vg7yXI" target="_blank">Video</a>
        </div>
        </div>
        </div>
        </div>

        <h3>2022</h3>
        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/abs/2206.10789" alt="Scaling Autoregressive Models for Content-Rich Text-to-Image Generation" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/parti-small.png" alt="Scaling Autoregressive Models for Content-Rich Text-to-Image Generation" /></a>
        </div>
        </a>
        <div class="col-md-9">
        <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Scaling Autoregressive Models for Content-Rich Text-to-Image Generation</p>
                <p class="paper-authors">Jiahui Yu, Yuanzhong Xu, <strong>Jing Yu Koh</strong>, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, Yonghui Wu</p>
                <p class="paper-venue"><i>TMLR</i>, 2022.</p>
            </div>
        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="http://parti.research.google" target="_blank">Website</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/abs/2206.10789" target="_blank">PDF</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://github.com/google-research/parti" target="_blank">GitHub</a>
        </div>
        </div>
        </div>
        </div>

        <h3>2021</h3>
        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/pdf/2105.08756.pdf" alt="Pathdreamer: A World Model for Indoor Navigation" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/pathdreamer-small.png" alt="Pathdreamer: A World Model for Indoor Navigation" /></a>
        </div>
        </a>
        <div class="col-md-9">
        <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Pathdreamer: A World Model for Indoor Navigation</p>
                <p class="paper-authors"><strong>Jing Yu Koh</strong>, Honglak Lee, Yinfei Yang, Jason Baldridge, Peter Anderson</p>
                <p class="paper-venue"><i>ICCV</i>, 2021.</p>
            </div>
        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://ai.googleblog.com/2021/09/pathdreamer-world-model-for-indoor.html" target="_blank">Blog Post</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://google-research.github.io/pathdreamer" target="_blank">Project Page</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/pdf/2105.08756.pdf" target="_blank">PDF</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://github.com/google-research/pathdreamer" target="_blank">Code</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://colab.research.google.com/drive/1ty3ib0K8BsFj5JH6Uq-GKGwXUl0ry8fA" target="_blank">Demo</a>
        <a class="btn btn-sm btn-outline-secondary" href="https://www.youtube.com/watch?v=StklIENGqs0" target="_blank">Video</a>
        </div>
        </div>
        </div>
        </div>

        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/pdf/2110.04627.pdf" alt="Vector-quantized Image Modeling with Improved VQGAN" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/vitvqgan.png" alt="Vector-quantized Image Modeling with Improved VQGAN" /></a>
        </div>
        </a>
        <div class="col-md-9">
        <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Vector-quantized Image Modeling with Improved VQGAN</p>
                <p class="paper-authors">Jiahui Yu, Xin Li, <strong>Jing Yu Koh</strong>, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, Yonghui Wu</p>
                <p class="paper-venue"><i>ICLR</i>, 2022.</p>
            </div>

        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/pdf/2110.04627.pdf" target="_blank">PDF</a>
        </div>
        </div>
        </div>
        </div>

        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/pdf/2101.04702.pdf" alt="Cross-Modal Contrastive Learning for Text-to-Image Generation" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/xmcgan.png" alt="Cross-Modal Contrastive Learning for Text-to-Image Generation" /></a>
        </div>
        </a>
        <div class="col-md-9">
        <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Cross-Modal Contrastive Learning for Text-to-Image Generation</p>
                <p class="paper-authors">Han Zhang<sup>*</sup>, <strong>Jing Yu Koh<sup>*</sup></strong>, Jason Baldridge, Honglak Lee, Yinfei Yang  (* denotes equal contribution)</p>
                <p class="paper-venue"><i>CVPR</i>, 2021.</p>
            </div>

        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/pdf/2101.04702.pdf" target="_blank">PDF</a> <a class="btn btn-sm btn-outline-secondary" href="https://github.com/google-research/xmcgan_image_generation" target="_blank">Code</a>
        </div>
        </div>
        </div>
        </div>

        <!-- <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/pdf/2104.06697.pdf" alt="Revisiting hierarchical approach for persistent long-term video prediction" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/hierarchical21.png" alt="Revisiting hierarchical approach for persistent long-term video prediction" /></a>
        </div>
        </a>
        <div class="col-md-9">
         <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Revisiting hierarchical approach for persistent long-term video prediction</p>
                <p class="paper-authors">Wonkwang Lee, Whie Jung, Han Zhang, Ting Chen, <strong>Jing Yu Koh</strong>, Thomas Huang, Hyungsuk Yoon, Honglak Lee, Seunghoon Hong</p>
                <p class="paper-venue">In <i>The International Conference on Learning Representations (ICLR)</i>, 2021.</p>
            </div>
       <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/pdf/2104.06697.pdf">PDF</a> <a class="btn btn-sm btn-outline-secondary" href="https://github.com/1Konny/HVP">Code</a>
        </div>
        </div>
        </div>
        </div> -->

        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/pdf/2011.03775.pdf" alt="Text-to-Image Generation Grounded by Fine-Grained User Attention" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/wacv21.png" alt="Text-to-Image Generation Grounded by Fine-Grained User Attention" /></a>
        </div>
        </a>
        <div class="col-md-9">
         <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Text-to-Image Generation Grounded by Fine-Grained User Attention</p>
                <p class="paper-authors"><strong>Jing Yu Koh</strong>, Jason Baldridge, Honglak Lee, Yinfei Yang</p>
                <p class="paper-venue"><i>WACV</i>, 2021.</p>
            </div>

        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/pdf/2011.03775.pdf" target="_blank">PDF</a> <a class="btn btn-sm btn-outline-secondary" href="https://github.com/google-research/trecs_image_generation" target="_blank">Dataset</a> <a class="btn btn-sm btn-outline-secondary" href="https://www.deeplearning.ai/the-batch/pictures-from-words-and-gestures/" target="_blank">The Batch Feature</a>
        </div>
        </div>
        </div>
        </div>

        <h3>2020</h3>
        <div class="row publication-div mb-4 box-shadow">
        <a href="https://arxiv.org/pdf/2002.02634" alt="SideInfNet" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/sideinfnet20.png" alt="SideInfNet" /></a>
        </div>
        </a>
        <div class="col-md-9">
         <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">SideInfNet: A Deep Neural Network for Semi-Automatic Semantic Segmentation with Side Information</p>
                <p class="paper-authors"><strong>Jing Yu Koh</strong>, Duc Thanh Nguyen, Quang-Trung Truong, Sai-Kit Yeung, Alexander Binder</p>
                <p class="paper-venue"><i>ECCV</i>, 2020.</p>
            </div>

        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://arxiv.org/pdf/2002.02634" target="_blank">PDF</a>
        <a class="btn btn-sm btn-outline-secondary" href="http://kohjingyu.com/sideinfnet/" target="_blank">Project Page</a>
        </div>
        </div>
        </div>
        </div>

        <!-- <h3>2019 and before</h3>
        <div class="row publication-div mb-4 box-shadow">
        <a target="_blank" href="https://www.ijcai.org/proceedings/2019/0813.pdf" alt="IJCAI 2019" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/ijcai19.png" alt="IJCAI 2019" /></a>
        </div>
        </a>
        <div class="col-md-9">
         <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Improving customer satisfaction in bike sharing systems through dynamic repositioning</p>
                <p class="paper-authors">Supriyo. Ghosh<sup>*</sup> <strong>Jing Yu Koh</strong><sup>*</sup>, Patrick Jaillet  (* denotes equal contribution)</p>
                <p class="paper-venue">In <i>International Joint Conference on Artificial Intelligence (IJCAI)</i>, 2019.</p>
            </div>

        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="https://www.ijcai.org/proceedings/2019/0813.pdf">PDF</a>
        </div>
        </div>
        </div>
        </div>

        <div class="row publication-div mb-4 box-shadow">
        <a target="_blank"  href="papers/Twitter_Informed_ICDM2018.pdf" alt="ICDM 2018" />
        <div class="col-md-3 publication-img-div">
        <img class="publication-img" src="papers/icdm18.png" alt="ICDM 2018" /></a>
        </div>
        </a>
        <div class="col-md-9">
         <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Twitter-informed Crowd Flow Prediction</p>
                <p class="paper-authors">Gary Goh, <strong>Jing Yu Koh</strong>, Yue Zhang</p>
                <p class="paper-venue">In <i>IEEE International Conference on Data Mining (ICDM) Workshops</i>, 2018.</p>
            </div>
        <div class="">
        <a class="btn btn-sm btn-outline-secondary" href="papers/Twitter_Informed_ICDM2018.pdf">PDF</a>
        </div>
        </div>
        </div>
        </div>

        <div class="row publication-div mb-4 box-shadow">
        <div class="col-md-3 publication-img-div">
        <a target="_blank" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Tian_Feng_Urban_Zoning_Using_ECCV_2018_paper.pdf"><img class="publication-img" src="papers/eccv18.png" alt="ECCV 2018"/></a>
        </div>
        <div class="col-md-9">
         <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Urban Zoning Using Higher-Order Markov Random Fields on Multi-View Imagery Data</p>
                <p class="paper-authors">Feng Tian, Quang-Trung Truong, Duc Thanh Nguyen, <strong>Jing Yu Koh</strong>, Lap-Fai Yu, Alexander Binder, Sai-Kit Yeung</p>
                <p class="paper-venue">In <i>The European Conference on Computer Vision (ECCV)</i>, 2018.</p>
            </div>

        <div class="">
        <a class="btn btn-sm btn-outline-secondary" target="_blank" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Tian_Feng_Urban_Zoning_Using_ECCV_2018_paper.pdf">PDF</a>
        </div>
        </div>
        </div>
        </div>

        <div class="row publication-div mb-4 box-shadow">
        <div class="col-md-3 publication-img-div">
        <a target="_blank" href="https://arxiv.org/pdf/1606.09187.pdf"><img class="publication-img" src="papers/gcpr17.png" alt="GCPR 2017" /></a>
        </div>
        <div class="col-md-9">
         <div class="paper-block">
            <div class="paper-details">
                <p class="paper-title">Object Boundary Detection and Classification</p>
                <p class="paper-authors"><strong>Jing Yu Koh</strong>, Wojciech Samek, Klaus-Robert Müller, Alexander Binder</p>
                <p class="paper-venue">In <i>39th German Conference on Pattern Recognition (GCPR)</i>, 2017.</p>
            </div>

        <div class="">
        <a class="btn btn-sm btn-outline-secondary" target="_blank" href="https://arxiv.org/pdf/1606.09187.pdf">PDF</a>
        <a class="btn btn-sm btn-outline-secondary" target="_blank" href="papers/gcpr17-poster.png">Poster</a>
        </div>
        </div>
        </div>
        </div> -->

    </div>


    </div>
    </main>

    <footer class="text-muted">
      <div class="container">
        <p class="float-right">
          <a href="#">Back to top</a>
        </p>
        <p>&copy; Jing Yu Koh 2022</p>
      </div>
    </footer>

    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
  </body>
</html>

